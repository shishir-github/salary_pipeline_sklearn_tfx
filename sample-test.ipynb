{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2327c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Text\n",
    "\n",
    "import absl\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.proto.orchestration import pipeline_pb2\n",
    "import tfx\n",
    "from tfx.orchestration.local.local_dag_runner import LocalDagRunner\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "\n",
    "\n",
    "_pipeline_name = 'salary__pipeline_sklearn_local'\n",
    "_root = os.path.dirname(__file__)\n",
    "_data_root = os.path.join(_root, 'data')\n",
    "\n",
    "_transform_module_file = os.path.join(_root, 'data_transform.py')\n",
    "\n",
    "_trainer_module_file = os.path.join(_root, 'salary_utils_sklearn.py')\n",
    "\n",
    "_evaluator_module_file = os.path.join(_root,\n",
    "                                      'sklearn_predict_extractor.py')\n",
    "\n",
    "_serving_model_dir = os.path.join(_root, 'serving_model',\n",
    "                                  _pipeline_name)\n",
    "\n",
    "_tfx_root = os.path.join(_root, 'tfx')\n",
    "\n",
    "_pipeline_root = os.path.join(_tfx_root, 'pipelines', _pipeline_name)\n",
    "# Sqlite ML-metadata db path.\n",
    "\n",
    "_metadata_path = os.path.join(_tfx_root, 'metadata', _pipeline_name,\n",
    "                              'metadata.db')\n",
    "\n",
    "_beam_pipeline_args = [\n",
    "    '--direct_running_mode=multi_threading',\n",
    "    # 0 means auto-detect based on on the number of CPUs available\n",
    "    # during execution time.\n",
    "    '--direct_num_workers=1',\n",
    "]\n",
    "\n",
    "\n",
    "def create_pipeline(\n",
    "    pipeline_name: Text,\n",
    "    pipeline_root: Text,\n",
    "    data_root: Text,\n",
    "    transform_module_file: Text,\n",
    "    trainer_module_file: Text,\n",
    "    #evaluator_module_file: Text,\n",
    "    serving_model_dir: Text,\n",
    "    metadata_path: Text,\n",
    "    beam_pipeline_args: List[Text],\n",
    "     ) -> pipeline.Pipeline:\n",
    "\n",
    "\n",
    "      \"\"\"Implements the Penguin pipeline with TFX.\"\"\"\n",
    "      # Brings data into the pipeline or otherwise joins/converts training data.\n",
    "    example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "      # Computes statistics over data for visualization and example validation.\n",
    "    statistics_gen = tfx.components.StatisticsGen(\n",
    "          examples=example_gen.outputs['examples'])\n",
    "\n",
    "      # Generates schema based on statistics files.\n",
    "    schema_gen = tfx.components.SchemaGen(\n",
    "          statistics=statistics_gen.outputs['statistics'],\n",
    "          infer_feature_shape=True)\n",
    "\n",
    "      # Performs anomaly detection based on statistics and data schema.\n",
    "    example_validator = tfx.components.ExampleValidator(\n",
    "          statistics=statistics_gen.outputs['statistics'],\n",
    "          schema=schema_gen.outputs['schema'])\n",
    "\n",
    "    transform  = tfx.components.Transform(\n",
    "            examples=example_gen.outputs['examples'],\n",
    "            schema=schema_gen.outputs['schema'],\n",
    "            module_file=transform_module_file\n",
    "        )\n",
    "\n",
    "    trainer = tfx.components.Trainer(\n",
    "          module_file=trainer_module_file,\n",
    "          examples=transform.outputs['transformed_examples'],\n",
    "          transform_graph=transform.outputs['transform_graph'],\n",
    "          schema=schema_gen.outputs['schema'],\n",
    "          train_args=trainer_pb2.TrainArgs(num_steps=2000),\n",
    "          eval_args=trainer_pb2.EvalArgs())\n",
    "\n",
    "    pusher = tfx.components.Pusher(\n",
    "          model=trainer.outputs['model'],\n",
    "          #model_blessing=evaluator.outputs['blessing'],\n",
    "          push_destination=pusher_pb2.PushDestination(\n",
    "              filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                  base_directory=serving_model_dir)))\n",
    "\n",
    "    return pipeline.Pipeline(\n",
    "          pipeline_name=pipeline_name,\n",
    "          pipeline_root=pipeline_root,\n",
    "          components=[\n",
    "              example_gen,\n",
    "              statistics_gen,\n",
    "              schema_gen,\n",
    "              example_validator,\n",
    "              transform,\n",
    "              trainer,\n",
    "              #model_resolver,\n",
    "              #evaluator,\n",
    "              pusher,\n",
    "          ],\n",
    "          enable_cache=True,\n",
    "          metadata_connection_config=tfx.orchestration.metadata.\n",
    "          sqlite_metadata_connection_config(metadata_path),\n",
    "          beam_pipeline_args=beam_pipeline_args,\n",
    "      )\n",
    "\n",
    "\n",
    "# To run this pipeline from the python CLI:\n",
    "#   $python penguin_pipeline_sklearn_local.py\n",
    "if __name__ == '__main__':\n",
    "    absl.logging.set_verbosity(absl.logging.INFO)\n",
    "    LocalDagRunner().run(\n",
    "      create_pipeline(pipeline_name=_pipeline_name,\n",
    "                       pipeline_root=_pipeline_root,\n",
    "                       data_root=_data_root,\n",
    "                       transform_module_file=_transform_module_file,\n",
    "                       trainer_module_file=_trainer_module_file,\n",
    "                       #evaluator_module_file=_evaluator_module_file,\n",
    "                       serving_model_dir=_serving_model_dir,\n",
    "                       metadata_path=_metadata_path,\n",
    "                       beam_pipeline_args=_beam_pipeline_args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198d8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Two files are needed\n",
    "# 1. is transformation file \n",
    "# 2. is trainer file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934f732",
   "metadata": {},
   "source": [
    "Lets define a Custome Compoenent that will be used in tranformation of the input data \n",
    "we will used Pandas for tranformation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74714bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Specifications for the compoenent (parameters, input and output)\n",
    "\n",
    "class PandasTranformComponentSpec(tfx.types.ComponentSpec):\n",
    "  \"\"\"ComponentSpec for Custom TFX Hello World Component.\"\"\"\n",
    "\n",
    "    PARAMETERS = {}\n",
    "    \n",
    "    INPUTS = {\n",
    "      'examples': tfx.ChannelParameter(type=standard_artifacts.Examples),\n",
    "  }\n",
    "    OUTPUTS = {\n",
    "      'augmented_data': tfx.ChannelParameter(type=standard_artifacts.Examples),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f6f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfx.v1 as tfx\n",
    "from tfx.dsl.component.experimental.decorators import component\n",
    "\n",
    "@component\n",
    "def MyTransformerComponent(\n",
    "    input_data: tfx.dsl.components.InputArtifact[tfx.types.standard_artifacts.Examples],\n",
    "    tranformed_data: tfx.dsl.components.OutputArtifact[tfx.types.standard_artifacts.Model],\n",
    "    ):\n",
    "  '''My simple trainer component.'''\n",
    "\n",
    "\n",
    "\n",
    "      return {\n",
    "        'loss': model_obj.loss,\n",
    "        'accuracy': model_obj.accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae992f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12112\\144948162.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexample_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'examples'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'example_gen' is not defined"
     ]
    }
   ],
   "source": [
    "example_gen.outputs['examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b3d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage in a pipeline graph definition:\n",
    "# ...\n",
    "trainer = MyTrainerComponent(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    dropout_hyperparameter=other_component.outputs['dropout'],\n",
    "    num_iterations=1000)\n",
    "pusher = Pusher(model=trainer.outputs['model'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
